{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "def encode_text_dummy(df,name):\n",
    "    dummies = pd.get_dummies(df[name])\n",
    "    for x in dummies.columns:\n",
    "        dummy_name = \"{}-{}\".format(name,x)\n",
    "        df[dummy_name] = dummies[x]\n",
    "    df.drop(name, axis=1, inplace=True)\n",
    "\n",
    "def encode_text_single_dummy(df,name,target_values):\n",
    "    for tv in target_values:\n",
    "        l = list(df[name].astype(str))\n",
    "        l = [1 if str(x)==str(tv) else 0 for x in l]\n",
    "        name2 = \"{}-{}\".format(name,tv)\n",
    "        df[name2] = l\n",
    "    \n",
    "def encode_text_index(df,name):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df[name] = le.fit_transform(df[name])\n",
    "    return le.classes_\n",
    "\n",
    "def encode_numeric_zscore(df,name,mean=None,sd=None):\n",
    "    if mean is None:\n",
    "        mean = df[name].mean()\n",
    "\n",
    "    if sd is None:\n",
    "        sd = df[name].std()\n",
    "\n",
    "    df[name] = (df[name]-mean)/sd\n",
    "\n",
    "def missing_median(df, name):\n",
    "    med = df[name].median()\n",
    "    df[name] = df[name].fillna(med)\n",
    "\n",
    "def to_xy(df,target):\n",
    "    result = []\n",
    "    for x in df.columns:\n",
    "        if x != target:\n",
    "            result.append(x)\n",
    "\n",
    "    target_type = df[target].dtypes\n",
    "    target_type = target_type[0] if hasattr(target_type, '__iter__') else target_type\n",
    "    \n",
    "    if target_type in (np.int64, np.int32):\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.int32)\n",
    "    else:\n",
    "        return df.as_matrix(result).astype(np.float32),df.as_matrix([target]).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.014460102654993534\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>b</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "      <th>chas-0</th>\n",
       "      <th>chas-1</th>\n",
       "      <th>pred</th>\n",
       "      <th>ideal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>-1.457558</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.074499</td>\n",
       "      <td>0.159528</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166084</td>\n",
       "      <td>0.159528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.491953</td>\n",
       "      <td>-0.101424</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.092114</td>\n",
       "      <td>-0.101424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>-0.302794</td>\n",
       "      <td>0.396035</td>\n",
       "      <td>-1.207532</td>\n",
       "      <td>1.322937</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.331941</td>\n",
       "      <td>1.322937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>-1.360171</td>\n",
       "      <td>1.181589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.182335</td>\n",
       "      <td>1.181589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-1.025487</td>\n",
       "      <td>1.486032</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.491460</td>\n",
       "      <td>1.486032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.416631</td>\n",
       "      <td>0.206892</td>\n",
       "      <td>-0.350810</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>0.112920</td>\n",
       "      <td>0.410165</td>\n",
       "      <td>-1.042291</td>\n",
       "      <td>0.670558</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.651690</td>\n",
       "      <td>0.670558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.409837</td>\n",
       "      <td>-0.388027</td>\n",
       "      <td>-0.070159</td>\n",
       "      <td>0.838414</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.426376</td>\n",
       "      <td>-0.031237</td>\n",
       "      <td>0.039925</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026682</td>\n",
       "      <td>0.039925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.403297</td>\n",
       "      <td>-0.160307</td>\n",
       "      <td>0.977841</td>\n",
       "      <td>1.023625</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.909800</td>\n",
       "      <td>0.496590</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.509440</td>\n",
       "      <td>0.496590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.395543</td>\n",
       "      <td>-0.930285</td>\n",
       "      <td>1.116390</td>\n",
       "      <td>1.086122</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.328123</td>\n",
       "      <td>2.419379</td>\n",
       "      <td>-0.655946</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.648712</td>\n",
       "      <td>-0.655946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.400333</td>\n",
       "      <td>-0.399413</td>\n",
       "      <td>0.615481</td>\n",
       "      <td>1.328320</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.622728</td>\n",
       "      <td>-0.394995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.383089</td>\n",
       "      <td>-0.394995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.393956</td>\n",
       "      <td>0.131459</td>\n",
       "      <td>0.913895</td>\n",
       "      <td>1.211780</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.392639</td>\n",
       "      <td>1.091846</td>\n",
       "      <td>-0.819041</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.810168</td>\n",
       "      <td>-0.819041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.406445</td>\n",
       "      <td>-0.392297</td>\n",
       "      <td>0.508905</td>\n",
       "      <td>1.154792</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.086393</td>\n",
       "      <td>-0.394995</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.396540</td>\n",
       "      <td>-0.394995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.409199</td>\n",
       "      <td>-0.563087</td>\n",
       "      <td>-1.050661</td>\n",
       "      <td>0.786365</td>\n",
       "      <td>-0.522484</td>\n",
       "      <td>-0.576948</td>\n",
       "      <td>-1.503749</td>\n",
       "      <td>0.370513</td>\n",
       "      <td>0.428079</td>\n",
       "      <td>-0.090551</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.081637</td>\n",
       "      <td>-0.090551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.346887</td>\n",
       "      <td>-0.477692</td>\n",
       "      <td>-0.240681</td>\n",
       "      <td>0.433325</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.615184</td>\n",
       "      <td>-0.231900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.217859</td>\n",
       "      <td>-0.231900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.345934</td>\n",
       "      <td>-0.268474</td>\n",
       "      <td>0.565746</td>\n",
       "      <td>0.316690</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.255721</td>\n",
       "      <td>-0.335113</td>\n",
       "      <td>-0.471106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.474761</td>\n",
       "      <td>-0.471105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.347162</td>\n",
       "      <td>-0.641365</td>\n",
       "      <td>-0.428966</td>\n",
       "      <td>0.334119</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.426595</td>\n",
       "      <td>-0.585776</td>\n",
       "      <td>-0.286265</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.267244</td>\n",
       "      <td>-0.286265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.297574</td>\n",
       "      <td>-0.497617</td>\n",
       "      <td>-1.395257</td>\n",
       "      <td>0.334119</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.330533</td>\n",
       "      <td>-0.850443</td>\n",
       "      <td>0.061671</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.079383</td>\n",
       "      <td>0.061671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.328932</td>\n",
       "      <td>-0.419338</td>\n",
       "      <td>0.466275</td>\n",
       "      <td>0.219811</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.329438</td>\n",
       "      <td>0.282442</td>\n",
       "      <td>-0.547216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.538746</td>\n",
       "      <td>-0.547216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.326780</td>\n",
       "      <td>-1.179354</td>\n",
       "      <td>-1.135922</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>-0.741378</td>\n",
       "      <td>-0.134863</td>\n",
       "      <td>-0.253646</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.248044</td>\n",
       "      <td>-0.253646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.335721</td>\n",
       "      <td>-0.793653</td>\n",
       "      <td>0.032865</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.375442</td>\n",
       "      <td>-0.192277</td>\n",
       "      <td>-0.471106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.477441</td>\n",
       "      <td>-0.471105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.274571</td>\n",
       "      <td>-1.017104</td>\n",
       "      <td>1.048891</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.217931</td>\n",
       "      <td>1.171666</td>\n",
       "      <td>-0.971263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.984447</td>\n",
       "      <td>-0.971263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.321045</td>\n",
       "      <td>-0.454920</td>\n",
       "      <td>0.732715</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.392749</td>\n",
       "      <td>0.164813</td>\n",
       "      <td>-0.318884</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.285472</td>\n",
       "      <td>-0.318884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.276817</td>\n",
       "      <td>-0.203004</td>\n",
       "      <td>0.821529</td>\n",
       "      <td>0.086364</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.849585</td>\n",
       "      <td>-0.797295</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.786940</td>\n",
       "      <td>-0.797295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.305189</td>\n",
       "      <td>-0.671254</td>\n",
       "      <td>1.116390</td>\n",
       "      <td>0.142544</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.414766</td>\n",
       "      <td>1.012026</td>\n",
       "      <td>-0.873406</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.872443</td>\n",
       "      <td>-0.873406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.332878</td>\n",
       "      <td>-0.513273</td>\n",
       "      <td>0.906790</td>\n",
       "      <td>0.287104</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.412465</td>\n",
       "      <td>0.510700</td>\n",
       "      <td>-0.753803</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.753772</td>\n",
       "      <td>-0.753803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.322382</td>\n",
       "      <td>-0.975829</td>\n",
       "      <td>0.608376</td>\n",
       "      <td>0.313223</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>-0.583319</td>\n",
       "      <td>0.540107</td>\n",
       "      <td>-0.938644</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.942882</td>\n",
       "      <td>-0.938644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.341987</td>\n",
       "      <td>-0.671254</td>\n",
       "      <td>0.771793</td>\n",
       "      <td>0.421215</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.221326</td>\n",
       "      <td>0.302047</td>\n",
       "      <td>-0.645073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.669198</td>\n",
       "      <td>-0.645073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.308986</td>\n",
       "      <td>-0.338213</td>\n",
       "      <td>0.718505</td>\n",
       "      <td>0.312653</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>-0.550897</td>\n",
       "      <td>0.647934</td>\n",
       "      <td>-0.840787</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.822340</td>\n",
       "      <td>-0.840787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.330235</td>\n",
       "      <td>0.299403</td>\n",
       "      <td>0.917447</td>\n",
       "      <td>0.313271</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.342472</td>\n",
       "      <td>0.020576</td>\n",
       "      <td>-0.449360</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.439220</td>\n",
       "      <td>-0.449360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.303559</td>\n",
       "      <td>0.554165</td>\n",
       "      <td>0.665217</td>\n",
       "      <td>0.210835</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>-0.600682</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.258021</td>\n",
       "      <td>-0.094253</td>\n",
       "      <td>-0.166662</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.164839</td>\n",
       "      <td>-0.166662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>0.146240</td>\n",
       "      <td>0.283747</td>\n",
       "      <td>0.889027</td>\n",
       "      <td>-0.707478</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.433058</td>\n",
       "      <td>0.843983</td>\n",
       "      <td>-0.634200</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.618319</td>\n",
       "      <td>-0.634200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>1.326491</td>\n",
       "      <td>-1.395688</td>\n",
       "      <td>1.020471</td>\n",
       "      <td>-0.804642</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>-0.078800</td>\n",
       "      <td>1.716403</td>\n",
       "      <td>-1.145231</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.155268</td>\n",
       "      <td>-1.145231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>0.769568</td>\n",
       "      <td>-0.141805</td>\n",
       "      <td>0.999156</td>\n",
       "      <td>-0.771494</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.252215</td>\n",
       "      <td>0.752960</td>\n",
       "      <td>-0.862533</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.864001</td>\n",
       "      <td>-0.862533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>1.246308</td>\n",
       "      <td>-0.079182</td>\n",
       "      <td>0.690085</td>\n",
       "      <td>-0.875639</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.291867</td>\n",
       "      <td>0.063987</td>\n",
       "      <td>-0.123170</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.105090</td>\n",
       "      <td>-0.123170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>0.256987</td>\n",
       "      <td>-0.060679</td>\n",
       "      <td>-0.137657</td>\n",
       "      <td>-0.176113</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.267896</td>\n",
       "      <td>0.050798</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056631</td>\n",
       "      <td>0.050798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>0.243521</td>\n",
       "      <td>0.662332</td>\n",
       "      <td>0.224702</td>\n",
       "      <td>-0.220041</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.398664</td>\n",
       "      <td>-0.688002</td>\n",
       "      <td>0.126909</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139005</td>\n",
       "      <td>0.126909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>0.246193</td>\n",
       "      <td>1.104963</td>\n",
       "      <td>0.299305</td>\n",
       "      <td>-0.182572</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.422871</td>\n",
       "      <td>-0.790228</td>\n",
       "      <td>0.268258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.259955</td>\n",
       "      <td>0.268258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>-0.092442</td>\n",
       "      <td>-0.743840</td>\n",
       "      <td>-1.004478</td>\n",
       "      <td>0.144017</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.397021</td>\n",
       "      <td>-0.312707</td>\n",
       "      <td>-0.079678</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073693</td>\n",
       "      <td>-0.079678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>-0.143573</td>\n",
       "      <td>-0.588705</td>\n",
       "      <td>-0.947637</td>\n",
       "      <td>-0.033738</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.153962</td>\n",
       "      <td>0.096195</td>\n",
       "      <td>-0.210154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.216277</td>\n",
       "      <td>-0.210154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.006993</td>\n",
       "      <td>0.038948</td>\n",
       "      <td>-0.592383</td>\n",
       "      <td>0.093392</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.349921</td>\n",
       "      <td>-0.290302</td>\n",
       "      <td>-0.144916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.122595</td>\n",
       "      <td>-0.144916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0.241611</td>\n",
       "      <td>-0.242855</td>\n",
       "      <td>0.398776</td>\n",
       "      <td>-0.118318</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.394392</td>\n",
       "      <td>0.325853</td>\n",
       "      <td>-0.373249</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.352127</td>\n",
       "      <td>-0.373249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>0.142085</td>\n",
       "      <td>-0.540315</td>\n",
       "      <td>-0.546200</td>\n",
       "      <td>-0.305238</td>\n",
       "      <td>1.659603</td>\n",
       "      <td>1.529413</td>\n",
       "      <td>0.805778</td>\n",
       "      <td>0.345539</td>\n",
       "      <td>-0.168471</td>\n",
       "      <td>-0.210154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.195221</td>\n",
       "      <td>-0.210154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>-0.402563</td>\n",
       "      <td>-1.182201</td>\n",
       "      <td>0.857054</td>\n",
       "      <td>-0.937519</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>1.796416</td>\n",
       "      <td>0.759588</td>\n",
       "      <td>0.420790</td>\n",
       "      <td>0.757161</td>\n",
       "      <td>-0.797295</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.795552</td>\n",
       "      <td>-0.797295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>-0.398783</td>\n",
       "      <td>-1.239131</td>\n",
       "      <td>1.055996</td>\n",
       "      <td>-0.968625</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>1.796416</td>\n",
       "      <td>0.759588</td>\n",
       "      <td>-0.138278</td>\n",
       "      <td>1.584770</td>\n",
       "      <td>-1.688880</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.672673</td>\n",
       "      <td>-1.688880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>-0.395983</td>\n",
       "      <td>-1.695994</td>\n",
       "      <td>1.045339</td>\n",
       "      <td>-0.936711</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>1.796416</td>\n",
       "      <td>0.759588</td>\n",
       "      <td>-0.418907</td>\n",
       "      <td>2.384371</td>\n",
       "      <td>-1.569277</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.558862</td>\n",
       "      <td>-1.569277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>-0.407809</td>\n",
       "      <td>-0.429301</td>\n",
       "      <td>1.073759</td>\n",
       "      <td>-0.915103</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>1.796416</td>\n",
       "      <td>0.759588</td>\n",
       "      <td>0.366242</td>\n",
       "      <td>0.758562</td>\n",
       "      <td>-0.971263</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.979613</td>\n",
       "      <td>-0.971263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>-0.407160</td>\n",
       "      <td>-0.429301</td>\n",
       "      <td>0.530220</td>\n",
       "      <td>-0.800273</td>\n",
       "      <td>-0.637331</td>\n",
       "      <td>1.796416</td>\n",
       "      <td>0.759588</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.097596</td>\n",
       "      <td>-0.264519</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.239834</td>\n",
       "      <td>-0.264519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>-0.399953</td>\n",
       "      <td>-0.822118</td>\n",
       "      <td>-0.517779</td>\n",
       "      <td>-0.671195</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.090051</td>\n",
       "      <td>-0.079678</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.067874</td>\n",
       "      <td>-0.079678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>-0.387599</td>\n",
       "      <td>-0.510426</td>\n",
       "      <td>-0.922769</td>\n",
       "      <td>-0.671195</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.131204</td>\n",
       "      <td>0.213893</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.215421</td>\n",
       "      <td>0.213893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>-0.399293</td>\n",
       "      <td>-0.874779</td>\n",
       "      <td>-1.413020</td>\n",
       "      <td>-0.473210</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.401074</td>\n",
       "      <td>0.692745</td>\n",
       "      <td>0.061671</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049062</td>\n",
       "      <td>0.061671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-0.386433</td>\n",
       "      <td>-1.273289</td>\n",
       "      <td>0.153651</td>\n",
       "      <td>-0.473210</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>1.188470</td>\n",
       "      <td>-0.308011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.305148</td>\n",
       "      <td>-0.308011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-0.388900</td>\n",
       "      <td>-0.698295</td>\n",
       "      <td>0.071942</td>\n",
       "      <td>-0.428522</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.202622</td>\n",
       "      <td>-0.460233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.457249</td>\n",
       "      <td>-0.460233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-0.392302</td>\n",
       "      <td>-0.378064</td>\n",
       "      <td>-0.116342</td>\n",
       "      <td>-0.658183</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.037381</td>\n",
       "      <td>-0.144916</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.148919</td>\n",
       "      <td>-0.144916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.399427</td>\n",
       "      <td>-1.018527</td>\n",
       "      <td>0.174966</td>\n",
       "      <td>-0.662552</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.428238</td>\n",
       "      <td>0.342657</td>\n",
       "      <td>-0.547216</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.541170</td>\n",
       "      <td>-0.547216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>-0.394016</td>\n",
       "      <td>-0.366678</td>\n",
       "      <td>0.395224</td>\n",
       "      <td>-0.615870</td>\n",
       "      <td>-0.407638</td>\n",
       "      <td>-0.102275</td>\n",
       "      <td>0.343873</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.234830</td>\n",
       "      <td>-0.623327</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.603690</td>\n",
       "      <td>-0.623327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>-0.412820</td>\n",
       "      <td>0.438881</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>-0.625178</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.386834</td>\n",
       "      <td>-0.417734</td>\n",
       "      <td>-0.014440</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.014072</td>\n",
       "      <td>-0.014440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>-0.414839</td>\n",
       "      <td>-0.234316</td>\n",
       "      <td>0.288648</td>\n",
       "      <td>-0.715931</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.500355</td>\n",
       "      <td>-0.210154</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.204234</td>\n",
       "      <td>-0.210154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>-0.413038</td>\n",
       "      <td>0.983986</td>\n",
       "      <td>0.796661</td>\n",
       "      <td>-0.772919</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.982076</td>\n",
       "      <td>0.148655</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155061</td>\n",
       "      <td>0.148655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>-0.407361</td>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.736268</td>\n",
       "      <td>-0.667776</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.402826</td>\n",
       "      <td>-0.864446</td>\n",
       "      <td>-0.057932</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.048401</td>\n",
       "      <td>-0.057932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>-0.414590</td>\n",
       "      <td>-0.362408</td>\n",
       "      <td>0.434302</td>\n",
       "      <td>-0.612640</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.802418</td>\n",
       "      <td>1.175303</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>-0.668397</td>\n",
       "      <td>-1.156104</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.141550</td>\n",
       "      <td>-1.156104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         crim        rm       age       dis       rad       tax   ptratio  \\\n",
       "0   -0.419367  0.413263 -0.119895  0.140075 -0.981871 -0.665949 -1.457558   \n",
       "1   -0.416927  0.194082  0.366803  0.556609 -0.867024 -0.986353 -0.302794   \n",
       "2   -0.416929  1.281446 -0.265549  0.556609 -0.867024 -0.986353 -0.302794   \n",
       "3   -0.416338  1.015298 -0.809088  1.076671 -0.752178 -1.105022  0.112920   \n",
       "4   -0.412074  1.227362 -0.510674  1.076671 -0.752178 -1.105022  0.112920   \n",
       "5   -0.416631  0.206892 -0.350810  1.076671 -0.752178 -1.105022  0.112920   \n",
       "6   -0.409837 -0.388027 -0.070159  0.838414 -0.522484 -0.576948 -1.503749   \n",
       "7   -0.403297 -0.160307  0.977841  1.023625 -0.522484 -0.576948 -1.503749   \n",
       "8   -0.395543 -0.930285  1.116390  1.086122 -0.522484 -0.576948 -1.503749   \n",
       "9   -0.400333 -0.399413  0.615481  1.328320 -0.522484 -0.576948 -1.503749   \n",
       "10  -0.393956  0.131459  0.913895  1.211780 -0.522484 -0.576948 -1.503749   \n",
       "11  -0.406445 -0.392297  0.508905  1.154792 -0.522484 -0.576948 -1.503749   \n",
       "12  -0.409199 -0.563087 -1.050661  0.786365 -0.522484 -0.576948 -1.503749   \n",
       "13  -0.346887 -0.477692 -0.240681  0.433325 -0.637331 -0.600682  1.175303   \n",
       "14  -0.345934 -0.268474  0.565746  0.316690 -0.637331 -0.600682  1.175303   \n",
       "15  -0.347162 -0.641365 -0.428966  0.334119 -0.637331 -0.600682  1.175303   \n",
       "16  -0.297574 -0.497617 -1.395257  0.334119 -0.637331 -0.600682  1.175303   \n",
       "17  -0.328932 -0.419338  0.466275  0.219811 -0.637331 -0.600682  1.175303   \n",
       "18  -0.326780 -1.179354 -1.135922  0.000692 -0.637331 -0.600682  1.175303   \n",
       "19  -0.335721 -0.793653  0.032865  0.000692 -0.637331 -0.600682  1.175303   \n",
       "20  -0.274571 -1.017104  1.048891  0.001357 -0.637331 -0.600682  1.175303   \n",
       "21  -0.321045 -0.454920  0.732715  0.103175 -0.637331 -0.600682  1.175303   \n",
       "22  -0.276817 -0.203004  0.821529  0.086364 -0.637331 -0.600682  1.175303   \n",
       "23  -0.305189 -0.671254  1.116390  0.142544 -0.637331 -0.600682  1.175303   \n",
       "24  -0.332878 -0.513273  0.906790  0.287104 -0.637331 -0.600682  1.175303   \n",
       "25  -0.322382 -0.975829  0.608376  0.313223 -0.637331 -0.600682  1.175303   \n",
       "26  -0.341987 -0.671254  0.771793  0.421215 -0.637331 -0.600682  1.175303   \n",
       "27  -0.308986 -0.338213  0.718505  0.312653 -0.637331 -0.600682  1.175303   \n",
       "28  -0.330235  0.299403  0.917447  0.313271 -0.637331 -0.600682  1.175303   \n",
       "29  -0.303559  0.554165  0.665217  0.210835 -0.637331 -0.600682  1.175303   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "476  0.146240  0.283747  0.889027 -0.707478  1.659603  1.529413  0.805778   \n",
       "477  1.326491 -1.395688  1.020471 -0.804642  1.659603  1.529413  0.805778   \n",
       "478  0.769568 -0.141805  0.999156 -0.771494  1.659603  1.529413  0.805778   \n",
       "479  1.246308 -0.079182  0.690085 -0.875639  1.659603  1.529413  0.805778   \n",
       "480  0.256987 -0.060679 -0.137657 -0.176113  1.659603  1.529413  0.805778   \n",
       "481  0.243521  0.662332  0.224702 -0.220041  1.659603  1.529413  0.805778   \n",
       "482  0.246193  1.104963  0.299305 -0.182572  1.659603  1.529413  0.805778   \n",
       "483 -0.092442 -0.743840 -1.004478  0.144017  1.659603  1.529413  0.805778   \n",
       "484 -0.143573 -0.588705 -0.947637 -0.033738  1.659603  1.529413  0.805778   \n",
       "485  0.006993  0.038948 -0.592383  0.093392  1.659603  1.529413  0.805778   \n",
       "486  0.241611 -0.242855  0.398776 -0.118318  1.659603  1.529413  0.805778   \n",
       "487  0.142085 -0.540315 -0.546200 -0.305238  1.659603  1.529413  0.805778   \n",
       "488 -0.402563 -1.182201  0.857054 -0.937519 -0.637331  1.796416  0.759588   \n",
       "489 -0.398783 -1.239131  1.055996 -0.968625 -0.637331  1.796416  0.759588   \n",
       "490 -0.395983 -1.695994  1.045339 -0.936711 -0.637331  1.796416  0.759588   \n",
       "491 -0.407809 -0.429301  1.073759 -0.915103 -0.637331  1.796416  0.759588   \n",
       "492 -0.407160 -0.429301  0.530220 -0.800273 -0.637331  1.796416  0.759588   \n",
       "493 -0.399953 -0.822118 -0.517779 -0.671195 -0.407638 -0.102275  0.343873   \n",
       "494 -0.387599 -0.510426 -0.922769 -0.671195 -0.407638 -0.102275  0.343873   \n",
       "495 -0.399293 -0.874779 -1.413020 -0.473210 -0.407638 -0.102275  0.343873   \n",
       "496 -0.386433 -1.273289  0.153651 -0.473210 -0.407638 -0.102275  0.343873   \n",
       "497 -0.388900 -0.698295  0.071942 -0.428522 -0.407638 -0.102275  0.343873   \n",
       "498 -0.392302 -0.378064 -0.116342 -0.658183 -0.407638 -0.102275  0.343873   \n",
       "499 -0.399427 -1.018527  0.174966 -0.662552 -0.407638 -0.102275  0.343873   \n",
       "500 -0.394016 -0.366678  0.395224 -0.615870 -0.407638 -0.102275  0.343873   \n",
       "501 -0.412820  0.438881  0.018654 -0.625178 -0.981871 -0.802418  1.175303   \n",
       "502 -0.414839 -0.234316  0.288648 -0.715931 -0.981871 -0.802418  1.175303   \n",
       "503 -0.413038  0.983986  0.796661 -0.772919 -0.981871 -0.802418  1.175303   \n",
       "504 -0.407361  0.724955  0.736268 -0.667776 -0.981871 -0.802418  1.175303   \n",
       "505 -0.414590 -0.362408  0.434302 -0.612640 -0.981871 -0.802418  1.175303   \n",
       "\n",
       "            b     lstat      medv  chas-0  chas-1      pred     ideal  \n",
       "0    0.440616 -1.074499  0.159528       1       0  0.166084  0.159528  \n",
       "1    0.440616 -0.491953 -0.101424       1       0 -0.092114 -0.101424  \n",
       "2    0.396035 -1.207532  1.322937       1       0  1.331941  1.322937  \n",
       "3    0.415751 -1.360171  1.181589       1       0  1.182335  1.181589  \n",
       "4    0.440616 -1.025487  1.486032       1       0  1.491460  1.486032  \n",
       "5    0.410165 -1.042291  0.670558       1       0  0.651690  0.670558  \n",
       "6    0.426376 -0.031237  0.039925       1       0  0.026682  0.039925  \n",
       "7    0.440616  0.909800  0.496590       1       0  0.509440  0.496590  \n",
       "8    0.328123  2.419379 -0.655946       1       0 -0.648712 -0.655946  \n",
       "9    0.329000  0.622728 -0.394995       1       0 -0.383089 -0.394995  \n",
       "10   0.392639  1.091846 -0.819041       1       0 -0.810168 -0.819041  \n",
       "11   0.440616  0.086393 -0.394995       1       0 -0.396540 -0.394995  \n",
       "12   0.370513  0.428079 -0.090551       1       0 -0.081637 -0.090551  \n",
       "13   0.440616 -0.615184 -0.231900       1       0 -0.217859 -0.231900  \n",
       "14   0.255721 -0.335113 -0.471106       1       0 -0.474761 -0.471105  \n",
       "15   0.426595 -0.585776 -0.286265       1       0 -0.267244 -0.286265  \n",
       "16   0.330533 -0.850443  0.061671       1       0  0.079383  0.061671  \n",
       "17   0.329438  0.282442 -0.547216       1       0 -0.538746 -0.547216  \n",
       "18  -0.741378 -0.134863 -0.253646       1       0 -0.248044 -0.253646  \n",
       "19   0.375442 -0.192277 -0.471106       1       0 -0.477441 -0.471105  \n",
       "20   0.217931  1.171666 -0.971263       1       0 -0.984447 -0.971263  \n",
       "21   0.392749  0.164813 -0.318884       1       0 -0.285472 -0.318884  \n",
       "22   0.440616  0.849585 -0.797295       1       0 -0.786940 -0.797295  \n",
       "23   0.414766  1.012026 -0.873406       1       0 -0.872443 -0.873406  \n",
       "24   0.412465  0.510700 -0.753803       1       0 -0.753772 -0.753803  \n",
       "25  -0.583319  0.540107 -0.938644       1       0 -0.942882 -0.938644  \n",
       "26   0.221326  0.302047 -0.645073       1       0 -0.669198 -0.645073  \n",
       "27  -0.550897  0.647934 -0.840787       1       0 -0.822340 -0.840787  \n",
       "28   0.342472  0.020576 -0.449360       1       0 -0.439220 -0.449360  \n",
       "29   0.258021 -0.094253 -0.166662       1       0 -0.164839 -0.166662  \n",
       "..        ...       ...       ...     ...     ...       ...       ...  \n",
       "476  0.433058  0.843983 -0.634200       1       0 -0.618319 -0.634200  \n",
       "477 -0.078800  1.716403 -1.145231       1       0 -1.155268 -1.145231  \n",
       "478  0.252215  0.752960 -0.862533       1       0 -0.864001 -0.862533  \n",
       "479  0.291867  0.063987 -0.123170       1       0 -0.105090 -0.123170  \n",
       "480  0.440616 -0.267896  0.050798       1       0  0.056631  0.050798  \n",
       "481  0.398664 -0.688002  0.126909       1       0  0.139005  0.126909  \n",
       "482  0.422871 -0.790228  0.268258       1       0  0.259955  0.268258  \n",
       "483  0.397021 -0.312707 -0.079678       1       0 -0.073693 -0.079678  \n",
       "484  0.153962  0.096195 -0.210154       1       0 -0.216277 -0.210154  \n",
       "485  0.349921 -0.290302 -0.144916       1       0 -0.122595 -0.144916  \n",
       "486  0.394392  0.325853 -0.373249       1       0 -0.352127 -0.373249  \n",
       "487  0.345539 -0.168471 -0.210154       1       0 -0.195221 -0.210154  \n",
       "488  0.420790  0.757161 -0.797295       1       0 -0.795552 -0.797295  \n",
       "489 -0.138278  1.584770 -1.688880       1       0 -1.672673 -1.688880  \n",
       "490 -0.418907  2.384371 -1.569277       1       0 -1.558862 -1.569277  \n",
       "491  0.366242  0.758562 -0.971263       1       0 -0.979613 -0.971263  \n",
       "492  0.440616  0.097596 -0.264519       1       0 -0.239834 -0.264519  \n",
       "493  0.440616 -0.090051 -0.079678       1       0 -0.067874 -0.079678  \n",
       "494  0.440616  0.131204  0.213893       1       0  0.215421  0.213893  \n",
       "495  0.401074  0.692745  0.061671       1       0  0.049062  0.061671  \n",
       "496  0.440616  1.188470 -0.308011       1       0 -0.305148 -0.308011  \n",
       "497  0.440616  0.202622 -0.460233       1       0 -0.457249 -0.460233  \n",
       "498  0.440616  0.037381 -0.144916       1       0 -0.148919 -0.144916  \n",
       "499  0.428238  0.342657 -0.547216       1       0 -0.541170 -0.547216  \n",
       "500  0.440616  0.234830 -0.623327       1       0 -0.603690 -0.623327  \n",
       "501  0.386834 -0.417734 -0.014440       1       0 -0.014072 -0.014440  \n",
       "502  0.440616 -0.500355 -0.210154       1       0 -0.204234 -0.210154  \n",
       "503  0.440616 -0.982076  0.148655       1       0  0.155061  0.148655  \n",
       "504  0.402826 -0.864446 -0.057932       1       0 -0.048401 -0.057932  \n",
       "505  0.440616 -0.668397 -1.156104       1       0 -1.141550 -1.156104  \n",
       "\n",
       "[506 rows x 14 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"housing.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "df.drop('nox',1,inplace=True)\n",
    "df.drop('zb',1,inplace=True)\n",
    "df.drop('indus',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'crim')\n",
    "#encode_numeric_zscore(df, 'zb')\n",
    "#encode_numeric_zscore(df, 'indus')\n",
    "#encode_numeric_zscore(df, 'nox')\n",
    "encode_numeric_zscore(df, 'rm')\n",
    "encode_numeric_zscore(df, 'age')\n",
    "encode_numeric_zscore(df, 'dis')\n",
    "encode_numeric_zscore(df, 'rad')\n",
    "encode_numeric_zscore(df, 'tax')\n",
    "encode_numeric_zscore(df, 'ptratio')\n",
    "encode_numeric_zscore(df, 'b')\n",
    "encode_numeric_zscore(df, 'lstat')\n",
    "encode_numeric_zscore(df, 'medv')\n",
    "encode_text_dummy(df, 'chas')\n",
    "\n",
    "df\n",
    "x,y = to_xy(df,['medv'])\n",
    "\n",
    "model_dir = 'tmp/medva' \n",
    "\n",
    "opt= tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=0.1)\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "regressor = skflow.DNNRegressor(\n",
    "    optimizer=opt,\n",
    "    model_dir= model_dir,\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[100, 50, 25])\n",
    "\n",
    "regressor.fit(x, y,steps=1000)\n",
    "\n",
    "pred = list(regressor.predict(x, as_iterable=True))\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
    "print(\"Final score (RMSE): {}\".format(score))\n",
    "\n",
    "pred = list(regressor.predict(x, as_iterable=True))\n",
    "predDF = pd.DataFrame(pred)\n",
    "df2 = pd.concat([df,predDF,pd.DataFrame(y)],axis=1)\n",
    "\n",
    "df2.columns = list(df.columns)+['pred','ideal']\n",
    "df2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNNRegressor(feature_columns=[_RealValuedColumn(column_name='', dimension=506, default_value=None, dtype=tf.float32, normalizer=None)], hidden_units=[100, 50, 25], optimizer=<tensorflow.python.training.adagrad.AdagradOptimizer object at 0x7f8d400d2358>, dropout=None)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"housing.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "df.drop('nox',1,inplace=True)\n",
    "df.drop('zb',1,inplace=True)\n",
    "df.drop('indus',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'crim')\n",
    "#encode_numeric_zscore(df, 'zb')\n",
    "#encode_numeric_zscore(df, 'indus')\n",
    "#encode_numeric_zscore(df, 'nox')\n",
    "encode_numeric_zscore(df, 'rm')\n",
    "encode_numeric_zscore(df, 'age')\n",
    "encode_numeric_zscore(df, 'dis')\n",
    "encode_numeric_zscore(df, 'rad')\n",
    "encode_numeric_zscore(df, 'tax')\n",
    "encode_numeric_zscore(df, 'ptratio')\n",
    "encode_numeric_zscore(df, 'b')\n",
    "encode_numeric_zscore(df, 'lstat')\n",
    "encode_numeric_zscore(df, 'medv')\n",
    "encode_text_dummy(df, 'chas')\n",
    "\n",
    "df\n",
    "\n",
    "x,y = to_xy(df,['medv'])\n",
    "\n",
    "model_dir = 'tmp/meb' \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "opt= tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=0.1)\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "regressor = skflow.DNNRegressor(\n",
    "    optimizer=opt,\n",
    "    model_dir= model_dir,\n",
    "    feature_columns=feature_columns,\n",
    "    hidden_units=[100, 50, 25])\n",
    "\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=500,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=50)\n",
    "    \n",
    "regressor.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (MSE): 0.001414737431332469\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "pred = list(regressor.predict(x_test, as_iterable=True))\n",
    "\n",
    "score = metrics.mean_squared_error(pred,y_test)\n",
    "print(\"Final score (MSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final score (RMSE): 0.037612996995449066\n"
     ]
    }
   ],
   "source": [
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Final score (RMSE): {}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (RMSE): 0.40426287055015564\n",
      "Fold #2\n",
      "Fold score (RMSE): 0.3568691611289978\n",
      "Fold #3\n",
      "Fold score (RMSE): 0.4648118019104004\n",
      "Fold #4\n",
      "Fold score (RMSE): 0.379515677690506\n",
      "Fold #5\n",
      "Fold score (RMSE): 0.309773325920105\n",
      "Final score (MSE): 0.09595952183008194\n",
      "Final, out of sample score (RMSE): 0.3865138292312622\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"housing.csv\")\n",
    "filename_write = os.path.join(path,\"housing-out-of-sample.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "\n",
    "df.drop('nox',1,inplace=True)\n",
    "df.drop('zb',1,inplace=True)\n",
    "df.drop('indus',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'crim')\n",
    "#encode_numeric_zscore(df, 'zb')\n",
    "#encode_numeric_zscore(df, 'indus')\n",
    "#encode_numeric_zscore(df, 'nox')\n",
    "encode_numeric_zscore(df, 'rm')\n",
    "encode_numeric_zscore(df, 'age')\n",
    "encode_numeric_zscore(df, 'dis')\n",
    "encode_numeric_zscore(df, 'rad')\n",
    "encode_numeric_zscore(df, 'tax')\n",
    "encode_numeric_zscore(df, 'ptratio')\n",
    "encode_numeric_zscore(df, 'b')\n",
    "encode_numeric_zscore(df, 'lstat')\n",
    "encode_numeric_zscore(df, 'medv')\n",
    "encode_text_dummy(df, 'chas')\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "x,y = to_xy(df,'medv')\n",
    "\n",
    "kf = KFold(5)\n",
    "    \n",
    "all_y = []\n",
    "all_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x):\n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x[train]\n",
    "    y_train = y[train]\n",
    "    x_test = x[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    model_dir = 'tmp/meKF' + str(fold)\n",
    "    \n",
    "    opt= tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=0.1)\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "    regressor = skflow.DNNRegressor(\n",
    "         optimizer=opt,\n",
    "         model_dir= model_dir,\n",
    "         feature_columns=feature_columns,\n",
    "         hidden_units=[100, 50, 25])\n",
    "\n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        every_n_steps=50,\n",
    "        early_stopping_metric=\"loss\",\n",
    "        early_stopping_metric_minimize=True,\n",
    "        early_stopping_rounds=50)\n",
    "    \n",
    "    regressor.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)\n",
    "\n",
    "    pred = list(regressor.predict(x_test, as_iterable=True))\n",
    "    \n",
    "    all_y.append(y_test)\n",
    "    all_pred.append(pred)        \n",
    "\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "\n",
    "\n",
    "all_y = np.concatenate(all_y)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "score1 = metrics.mean_squared_error(pred,y_test)\n",
    "print(\"Final score (MSE): {}\".format(score1))\n",
    "score = np.sqrt(metrics.mean_squared_error(all_pred,all_y))\n",
    "print(\"Final, out of sample score (RMSE): {}\".format(score)) \n",
    "\n",
    "all_y = pd.DataFrame(all_y)\n",
    "all_pred = pd.DataFrame(all_pred)\n",
    "allDF = pd.concat( [df, all_y, all_pred],axis=1 )\n",
    "allDF.to_csv(filename_write,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Fold score (MSE): 0.11733327805995941\n",
      "Fold score (RMSE): 0.3425394594669342\n",
      "Fold #2\n",
      "Fold score (MSE): 0.14659762382507324\n",
      "Fold score (RMSE): 0.38288068771362305\n",
      "Fold #3\n",
      "Fold score (MSE): 0.19778668880462646\n",
      "Fold score (RMSE): 0.4447321593761444\n",
      "Fold #4\n",
      "Fold score (MSE): 0.10027521848678589\n",
      "Fold score (RMSE): 0.31666263937950134\n",
      "Fold #5\n",
      "Fold score (MSE): 0.16740913689136505\n",
      "Fold score (RMSE): 0.4091566205024719\n",
      "\n",
      "Cross-validated score (RMSE): 0.3819429278373718\n",
      "Holdout score (RMSE): 0.41510075330734253\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.contrib.learn as learn\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "filename_read = os.path.join(path,\"housing.csv\")\n",
    "filename_write = os.path.join(path,\"housing-out-of-sample.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "\n",
    "df.drop('nox',1,inplace=True)\n",
    "df.drop('zb',1,inplace=True)\n",
    "df.drop('indus',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'crim')\n",
    "#encode_numeric_zscore(df, 'zb')\n",
    "#encode_numeric_zscore(df, 'indus')\n",
    "#encode_numeric_zscore(df, 'nox')\n",
    "encode_numeric_zscore(df, 'rm')\n",
    "encode_numeric_zscore(df, 'age')\n",
    "encode_numeric_zscore(df, 'dis')\n",
    "encode_numeric_zscore(df, 'rad')\n",
    "encode_numeric_zscore(df, 'tax')\n",
    "encode_numeric_zscore(df, 'ptratio')\n",
    "encode_numeric_zscore(df, 'b')\n",
    "encode_numeric_zscore(df, 'lstat')\n",
    "encode_numeric_zscore(df, 'medv')\n",
    "encode_text_dummy(df, 'chas')\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "x,y = to_xy(df,'medv')\n",
    "\n",
    "x_main, x_holdout, y_main, y_holdout = train_test_split(    \n",
    "    x, y, test_size=0.10) \n",
    "\n",
    "kf = KFold(5)\n",
    "    \n",
    "all_y = []\n",
    "all_pred = []\n",
    "fold = 0\n",
    "for train, test in kf.split(x_main):        \n",
    "    fold+=1\n",
    "    print(\"Fold #{}\".format(fold))\n",
    "        \n",
    "    x_train = x_main[train]\n",
    "    y_train = y_main[train]\n",
    "    x_test = x_main[test]\n",
    "    y_test = y_main[test]\n",
    "    \n",
    "    model_dir = 'tmp/meKFHS' + str(fold)\n",
    "    \n",
    "    opt= tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=0.1)\n",
    "    feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "    regressor = skflow.DNNRegressor(\n",
    "         optimizer=opt,\n",
    "         model_dir= model_dir,\n",
    "         feature_columns=feature_columns,\n",
    "         hidden_units=[100, 50, 25])\n",
    "\n",
    "    validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "        x_test,\n",
    "        y_test,\n",
    "        every_n_steps=50,\n",
    "        early_stopping_metric=\"loss\",\n",
    "        early_stopping_metric_minimize=True,\n",
    "        early_stopping_rounds=50)\n",
    "    \n",
    "    regressor.fit(x_train, y_train,monitors=[validation_monitor],steps=10000)\n",
    "\n",
    "    pred = list(regressor.predict(x_test, as_iterable=True))\n",
    "    \n",
    "    all_y.append(y_test)\n",
    "    all_pred.append(pred)        \n",
    "    score1 = metrics.mean_squared_error(pred,y_test)\n",
    "    print(\"Fold score (MSE): {}\".format(score1))\n",
    "    score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "    print(\"Fold score (RMSE): {}\".format(score))\n",
    "   \n",
    "\n",
    "all_y = np.concatenate(all_y)\n",
    "all_pred = np.concatenate(all_pred)\n",
    "score = np.sqrt(metrics.mean_squared_error(all_pred,all_y))\n",
    "print()\n",
    "print(\"Cross-validated score (RMSE): {}\".format(score))    \n",
    "    \n",
    "holdout_pred = list(regressor.predict(x_holdout, as_iterable=True))\n",
    "\n",
    "score = np.sqrt(metrics.mean_squared_error(holdout_pred,y_holdout))\n",
    "print(\"Holdout score (RMSE): {}\".format(score)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chart_regression(pred,y):\n",
    "    t = pd.DataFrame({'pred' : pred, 'y' : y_test.flatten()})\n",
    "    t.sort_values(by=['y'],inplace=True)\n",
    "    a = plt.plot(t['y'].tolist(),label='expected')\n",
    "    b = plt.plot(t['pred'].tolist(),label='prediction')\n",
    "    plt.ylabel('output')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (MSE): 0.1741606593132019\n",
      "Score (RMSE): 0.41732561588287354\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8d380872b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure, show\n",
    "import tensorflow.contrib.learn as learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from scipy.stats import zscore\n",
    "\n",
    "path = \"./data/\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "filename_read = os.path.join(path,\"housing.csv\")\n",
    "filename_write = os.path.join(path,\"housing-out-of-sample.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "\n",
    "\n",
    "df.drop('nox',1,inplace=True)\n",
    "df.drop('zb',1,inplace=True)\n",
    "df.drop('indus',1,inplace=True)\n",
    "encode_numeric_zscore(df, 'crim')\n",
    "#encode_numeric_zscore(df, 'zb')\n",
    "#encode_numeric_zscore(df, 'indus')\n",
    "#encode_numeric_zscore(df, 'nox')\n",
    "encode_numeric_zscore(df, 'rm')\n",
    "encode_numeric_zscore(df, 'age')\n",
    "encode_numeric_zscore(df, 'dis')\n",
    "encode_numeric_zscore(df, 'rad')\n",
    "encode_numeric_zscore(df, 'tax')\n",
    "encode_numeric_zscore(df, 'ptratio')\n",
    "encode_numeric_zscore(df, 'b')\n",
    "encode_numeric_zscore(df, 'lstat')\n",
    "encode_numeric_zscore(df, 'medv')\n",
    "encode_text_dummy(df, 'chas')\n",
    "\n",
    "\n",
    "x,y = to_xy(df,'medv')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.20, random_state=42)\n",
    "\n",
    "model_dir = 'tmp/mec' \n",
    "\n",
    "opt= tf.train.AdagradOptimizer(learning_rate=0.1, initial_accumulator_value=0.1)\n",
    "feature_columns = [tf.contrib.layers.real_valued_column(\"\", dimension=x.shape[0])]\n",
    "regressor = skflow.DNNRegressor(\n",
    "     optimizer=opt,\n",
    "     model_dir= model_dir,\n",
    "     feature_columns=feature_columns,\n",
    "     hidden_units=[100, 50, 25])\n",
    "\n",
    "validation_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    every_n_steps=100,\n",
    "    early_stopping_metric=\"loss\",\n",
    "    early_stopping_metric_minimize=True,\n",
    "    early_stopping_rounds=100)\n",
    "    \n",
    "regressor.fit(x_train, y_train,monitors=[validation_monitor],batch_size=32,steps=10000)\n",
    "\n",
    "pred = list(regressor.predict(x_test, as_iterable=True))\n",
    "score1 = metrics.mean_squared_error(pred,y_test)\n",
    "print(\"Score (MSE): {}\".format(score1))\n",
    "score = np.sqrt(metrics.mean_squared_error(pred,y_test))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "\n",
    "chart_regression(pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
