{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.classify import NaiveBayesClassifier  \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "from sklearn import naive_bayes\n",
    "from sklearn import svm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1956967341</td>\n",
       "      <td>empty</td>\n",
       "      <td>xoshayzers</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>wannamama</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>coolfunky</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>czareaquino</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>xkilljoyx</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>xxxPEACHESxxx</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1956968487</td>\n",
       "      <td>sadness</td>\n",
       "      <td>ShansBee</td>\n",
       "      <td>I should be sleep, but im not! thinking about ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1956968636</td>\n",
       "      <td>worry</td>\n",
       "      <td>mcsleazy</td>\n",
       "      <td>Hmmm. http://www.djhero.com/ is down</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1956969035</td>\n",
       "      <td>sadness</td>\n",
       "      <td>nic0lepaula</td>\n",
       "      <td>@charviray Charlene my love. I miss you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1956969172</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Ingenue_Em</td>\n",
       "      <td>@kelcouch I'm sorry  at least it's Friday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1956969456</td>\n",
       "      <td>neutral</td>\n",
       "      <td>feinyheiny</td>\n",
       "      <td>cant fall asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1956969531</td>\n",
       "      <td>worry</td>\n",
       "      <td>dudeitsmanda</td>\n",
       "      <td>Choked on her retainers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1956970047</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Danied32</td>\n",
       "      <td>Ugh! I have to beat this stupid song to get to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1956970424</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Samm_xo</td>\n",
       "      <td>@BrodyJenner if u watch the hills in london u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1956970860</td>\n",
       "      <td>surprise</td>\n",
       "      <td>okiepeanut93</td>\n",
       "      <td>Got the news</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1956971077</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Sim_34</td>\n",
       "      <td>The storm is here and the electricity is gone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1956971170</td>\n",
       "      <td>love</td>\n",
       "      <td>poppygallico</td>\n",
       "      <td>@annarosekerr agreed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1956971206</td>\n",
       "      <td>sadness</td>\n",
       "      <td>brokenangel1982</td>\n",
       "      <td>So sleepy again and it's not even that late. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1956971473</td>\n",
       "      <td>worry</td>\n",
       "      <td>LCJ82</td>\n",
       "      <td>@PerezHilton lady gaga tweeted about not being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1956971586</td>\n",
       "      <td>sadness</td>\n",
       "      <td>cleepow</td>\n",
       "      <td>How are YOU convinced that I have always wante...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1956971981</td>\n",
       "      <td>worry</td>\n",
       "      <td>andreagauster</td>\n",
       "      <td>@raaaaaaek oh too bad! I hope it gets better. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1956972097</td>\n",
       "      <td>fun</td>\n",
       "      <td>schiz0phren1c</td>\n",
       "      <td>Wondering why I'm awake at 7am,writing a new s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1956972116</td>\n",
       "      <td>neutral</td>\n",
       "      <td>jansc</td>\n",
       "      <td>No Topic Maps talks at the Balisage Markup Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1956972270</td>\n",
       "      <td>worry</td>\n",
       "      <td>sweet8181</td>\n",
       "      <td>I ate Something I don't know what it is... Why...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1956972359</td>\n",
       "      <td>sadness</td>\n",
       "      <td>xamountoftruth</td>\n",
       "      <td>so tired and i think i'm definitely going to g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1956972444</td>\n",
       "      <td>worry</td>\n",
       "      <td>jomama6881</td>\n",
       "      <td>On my way home n having 2 deal w underage girl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1956972557</td>\n",
       "      <td>sadness</td>\n",
       "      <td>LilithGaea</td>\n",
       "      <td>@IsaacMascote  i'm sorry people are so rude to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1956972884</td>\n",
       "      <td>worry</td>\n",
       "      <td>oONEPTUNEOo</td>\n",
       "      <td>Damm servers still down  i need to hit 80 befo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1956973598</td>\n",
       "      <td>sadness</td>\n",
       "      <td>username_origin</td>\n",
       "      <td>Fudge.... Just BS'd that whole paper.... So ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1956973690</td>\n",
       "      <td>worry</td>\n",
       "      <td>catchtheapple</td>\n",
       "      <td>I HATE CANCER. I HATE IT I HATE IT I HATE IT.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39970</th>\n",
       "      <td>1753904518</td>\n",
       "      <td>love</td>\n",
       "      <td>lisa24270</td>\n",
       "      <td>@Rtib happy birthday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39971</th>\n",
       "      <td>1753904526</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>Freakonomy</td>\n",
       "      <td>@SarahSaner Hey Sarah! Hws u? Hope u remember me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39972</th>\n",
       "      <td>1753904626</td>\n",
       "      <td>happiness</td>\n",
       "      <td>tchvinkle</td>\n",
       "      <td>@acchanosaurus good luck chan! gue kmrn bawa b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39973</th>\n",
       "      <td>1753904668</td>\n",
       "      <td>fun</td>\n",
       "      <td>Mirunnetje</td>\n",
       "      <td>good morning/midday nation!  FORMULA ONE IN ON...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39974</th>\n",
       "      <td>1753904674</td>\n",
       "      <td>love</td>\n",
       "      <td>DivasMistress</td>\n",
       "      <td>to my pretty lady @nikkiwoods HAPPY MOTHER'S D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39975</th>\n",
       "      <td>1753904682</td>\n",
       "      <td>empty</td>\n",
       "      <td>patphelan</td>\n",
       "      <td>@lexia Or even listen to Susan's green policies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39976</th>\n",
       "      <td>1753904748</td>\n",
       "      <td>neutral</td>\n",
       "      <td>CharlotteMcFLY</td>\n",
       "      <td>right. coursework now. PROMISE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39977</th>\n",
       "      <td>1753904799</td>\n",
       "      <td>surprise</td>\n",
       "      <td>Ocnarf10</td>\n",
       "      <td>@BuddingGenius you dont say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39978</th>\n",
       "      <td>1753904868</td>\n",
       "      <td>worry</td>\n",
       "      <td>theknickermafia</td>\n",
       "      <td>@givemestrength bloody Feds, they lost last st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39979</th>\n",
       "      <td>1753904911</td>\n",
       "      <td>surprise</td>\n",
       "      <td>fadedmoon</td>\n",
       "      <td>@prinsezha awesome. Wha'dya get her?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39980</th>\n",
       "      <td>1753904912</td>\n",
       "      <td>happiness</td>\n",
       "      <td>RyanJL</td>\n",
       "      <td>Sitting in Gatwick- going home for a week! can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39981</th>\n",
       "      <td>1753904919</td>\n",
       "      <td>happiness</td>\n",
       "      <td>DominiqueGoh</td>\n",
       "      <td>@maynaseric good luck with your auction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39982</th>\n",
       "      <td>1753905020</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2str20lt</td>\n",
       "      <td>hey guys, if you have something to ask, just a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39983</th>\n",
       "      <td>1753905073</td>\n",
       "      <td>neutral</td>\n",
       "      <td>ABZQuine</td>\n",
       "      <td>@Astronick not really just leaving flat now, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39984</th>\n",
       "      <td>1753905113</td>\n",
       "      <td>surprise</td>\n",
       "      <td>kanjigirl</td>\n",
       "      <td>@iscreamshinki Oh that's why.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39985</th>\n",
       "      <td>1753905121</td>\n",
       "      <td>happiness</td>\n",
       "      <td>MizFitOnline</td>\n",
       "      <td>@McMedia husband is golfing &amp;amp; the Toddler ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39986</th>\n",
       "      <td>1753905153</td>\n",
       "      <td>happiness</td>\n",
       "      <td>sue_jr</td>\n",
       "      <td>going to watch boy in the striped pj's hope i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39987</th>\n",
       "      <td>1753918809</td>\n",
       "      <td>happiness</td>\n",
       "      <td>njohari</td>\n",
       "      <td>gave the bikes a thorough wash, degrease it an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39988</th>\n",
       "      <td>1753918818</td>\n",
       "      <td>happiness</td>\n",
       "      <td>RachellSmithles</td>\n",
       "      <td>had SUCH and AMAZING time last night, McFly we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39989</th>\n",
       "      <td>1753918822</td>\n",
       "      <td>love</td>\n",
       "      <td>Rellz</td>\n",
       "      <td>His snoring is so annoying n it keeps me from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39990</th>\n",
       "      <td>1753918829</td>\n",
       "      <td>neutral</td>\n",
       "      <td>kdpaine</td>\n",
       "      <td>@shonali I think the lesson of the day is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39991</th>\n",
       "      <td>1753918846</td>\n",
       "      <td>neutral</td>\n",
       "      <td>x0159432</td>\n",
       "      <td>@lovelylisaj can you give me the link for the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39992</th>\n",
       "      <td>1753918881</td>\n",
       "      <td>neutral</td>\n",
       "      <td>_Alectrona_</td>\n",
       "      <td>@jasimmo Ooo showing of your French skills!! l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39993</th>\n",
       "      <td>1753918892</td>\n",
       "      <td>neutral</td>\n",
       "      <td>bushidosan</td>\n",
       "      <td>@sendsome2me haha, yeah. Twitter has many uses...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39994</th>\n",
       "      <td>1753918900</td>\n",
       "      <td>happiness</td>\n",
       "      <td>courtside101</td>\n",
       "      <td>Succesfully following Tayla!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>showMe_Heaven</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>drapeaux</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>JenniRox</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>ipdaman1</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>Alpharalpha</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment           author  \\\n",
       "0      1956967341       empty       xoshayzers   \n",
       "1      1956967666     sadness        wannamama   \n",
       "2      1956967696     sadness        coolfunky   \n",
       "3      1956967789  enthusiasm      czareaquino   \n",
       "4      1956968416     neutral        xkilljoyx   \n",
       "5      1956968477       worry    xxxPEACHESxxx   \n",
       "6      1956968487     sadness         ShansBee   \n",
       "7      1956968636       worry         mcsleazy   \n",
       "8      1956969035     sadness      nic0lepaula   \n",
       "9      1956969172     sadness       Ingenue_Em   \n",
       "10     1956969456     neutral       feinyheiny   \n",
       "11     1956969531       worry     dudeitsmanda   \n",
       "12     1956970047     sadness         Danied32   \n",
       "13     1956970424     sadness          Samm_xo   \n",
       "14     1956970860    surprise     okiepeanut93   \n",
       "15     1956971077     sadness           Sim_34   \n",
       "16     1956971170        love     poppygallico   \n",
       "17     1956971206     sadness  brokenangel1982   \n",
       "18     1956971473       worry            LCJ82   \n",
       "19     1956971586     sadness          cleepow   \n",
       "20     1956971981       worry    andreagauster   \n",
       "21     1956972097         fun    schiz0phren1c   \n",
       "22     1956972116     neutral            jansc   \n",
       "23     1956972270       worry        sweet8181   \n",
       "24     1956972359     sadness   xamountoftruth   \n",
       "25     1956972444       worry       jomama6881   \n",
       "26     1956972557     sadness       LilithGaea   \n",
       "27     1956972884       worry      oONEPTUNEOo   \n",
       "28     1956973598     sadness  username_origin   \n",
       "29     1956973690       worry    catchtheapple   \n",
       "...           ...         ...              ...   \n",
       "39970  1753904518        love        lisa24270   \n",
       "39971  1753904526  enthusiasm       Freakonomy   \n",
       "39972  1753904626   happiness        tchvinkle   \n",
       "39973  1753904668         fun       Mirunnetje   \n",
       "39974  1753904674        love    DivasMistress   \n",
       "39975  1753904682       empty        patphelan   \n",
       "39976  1753904748     neutral   CharlotteMcFLY   \n",
       "39977  1753904799    surprise         Ocnarf10   \n",
       "39978  1753904868       worry  theknickermafia   \n",
       "39979  1753904911    surprise        fadedmoon   \n",
       "39980  1753904912   happiness           RyanJL   \n",
       "39981  1753904919   happiness     DominiqueGoh   \n",
       "39982  1753905020     neutral         2str20lt   \n",
       "39983  1753905073     neutral         ABZQuine   \n",
       "39984  1753905113    surprise        kanjigirl   \n",
       "39985  1753905121   happiness     MizFitOnline   \n",
       "39986  1753905153   happiness           sue_jr   \n",
       "39987  1753918809   happiness          njohari   \n",
       "39988  1753918818   happiness  RachellSmithles   \n",
       "39989  1753918822        love            Rellz   \n",
       "39990  1753918829     neutral          kdpaine   \n",
       "39991  1753918846     neutral         x0159432   \n",
       "39992  1753918881     neutral      _Alectrona_   \n",
       "39993  1753918892     neutral       bushidosan   \n",
       "39994  1753918900   happiness     courtside101   \n",
       "39995  1753918954     neutral    showMe_Heaven   \n",
       "39996  1753919001        love         drapeaux   \n",
       "39997  1753919005        love         JenniRox   \n",
       "39998  1753919043   happiness         ipdaman1   \n",
       "39999  1753919049        love      Alpharalpha   \n",
       "\n",
       "                                                 content  \n",
       "0      @tiffanylue i know  i was listenin to bad habi...  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "5      Re-pinging @ghostridah14: why didn't you go to...  \n",
       "6      I should be sleep, but im not! thinking about ...  \n",
       "7                   Hmmm. http://www.djhero.com/ is down  \n",
       "8                @charviray Charlene my love. I miss you  \n",
       "9             @kelcouch I'm sorry  at least it's Friday?  \n",
       "10                                      cant fall asleep  \n",
       "11                               Choked on her retainers  \n",
       "12     Ugh! I have to beat this stupid song to get to...  \n",
       "13     @BrodyJenner if u watch the hills in london u ...  \n",
       "14                                          Got the news  \n",
       "15         The storm is here and the electricity is gone  \n",
       "16                                  @annarosekerr agreed  \n",
       "17     So sleepy again and it's not even that late. I...  \n",
       "18     @PerezHilton lady gaga tweeted about not being...  \n",
       "19     How are YOU convinced that I have always wante...  \n",
       "20     @raaaaaaek oh too bad! I hope it gets better. ...  \n",
       "21     Wondering why I'm awake at 7am,writing a new s...  \n",
       "22     No Topic Maps talks at the Balisage Markup Con...  \n",
       "23     I ate Something I don't know what it is... Why...  \n",
       "24     so tired and i think i'm definitely going to g...  \n",
       "25     On my way home n having 2 deal w underage girl...  \n",
       "26     @IsaacMascote  i'm sorry people are so rude to...  \n",
       "27     Damm servers still down  i need to hit 80 befo...  \n",
       "28     Fudge.... Just BS'd that whole paper.... So ti...  \n",
       "29         I HATE CANCER. I HATE IT I HATE IT I HATE IT.  \n",
       "...                                                  ...  \n",
       "39970                               @Rtib happy birthday  \n",
       "39971   @SarahSaner Hey Sarah! Hws u? Hope u remember me  \n",
       "39972  @acchanosaurus good luck chan! gue kmrn bawa b...  \n",
       "39973  good morning/midday nation!  FORMULA ONE IN ON...  \n",
       "39974  to my pretty lady @nikkiwoods HAPPY MOTHER'S D...  \n",
       "39975    @lexia Or even listen to Susan's green policies  \n",
       "39976                     right. coursework now. PROMISE  \n",
       "39977                        @BuddingGenius you dont say  \n",
       "39978  @givemestrength bloody Feds, they lost last st...  \n",
       "39979               @prinsezha awesome. Wha'dya get her?  \n",
       "39980  Sitting in Gatwick- going home for a week! can...  \n",
       "39981            @maynaseric good luck with your auction  \n",
       "39982  hey guys, if you have something to ask, just a...  \n",
       "39983  @Astronick not really just leaving flat now, o...  \n",
       "39984                      @iscreamshinki Oh that's why.  \n",
       "39985  @McMedia husband is golfing &amp; the Toddler ...  \n",
       "39986  going to watch boy in the striped pj's hope i ...  \n",
       "39987  gave the bikes a thorough wash, degrease it an...  \n",
       "39988  had SUCH and AMAZING time last night, McFly we...  \n",
       "39989  His snoring is so annoying n it keeps me from ...  \n",
       "39990  @shonali I think the lesson of the day is not ...  \n",
       "39991  @lovelylisaj can you give me the link for the ...  \n",
       "39992  @jasimmo Ooo showing of your French skills!! l...  \n",
       "39993  @sendsome2me haha, yeah. Twitter has many uses...  \n",
       "39994                      Succesfully following Tayla!!  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[40000 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the data\n",
    "path=\"C:/Users/dawit/Google Drive/Data Mining/project\"\n",
    "filename_read = os.path.join(path,\"text_emotion.csv\")\n",
    "df = pd.read_csv(filename_read,na_values=['NA','?'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...\n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...\n",
       "2     sadness                Funeral ceremony...gloomy friday...\n",
       "3  enthusiasm               wants to hang out with friends SOON!\n",
       "4     neutral  @dannycastillo We want to trade with someone w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop('tweet_id', axis= 1)\n",
    "df=df.drop('author', axis= 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>Emotion level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>empty</td>\n",
       "      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentiment                                            content  \\\n",
       "0       empty  @tiffanylue i know  i was listenin to bad habi...   \n",
       "1     sadness  Layin n bed with a headache  ughhhh...waitin o...   \n",
       "2     sadness                Funeral ceremony...gloomy friday...   \n",
       "3  enthusiasm               wants to hang out with friends SOON!   \n",
       "4     neutral  @dannycastillo We want to trade with someone w...   \n",
       "\n",
       "   Emotion level  \n",
       "0              0  \n",
       "1              1  \n",
       "2              1  \n",
       "3             10  \n",
       "4              6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Emotion level'] = np.where(df['sentiment'].str.contains('sadness'), 1,np.where(df['sentiment'].str.contains('worry'), 2,np.where(df['sentiment'].str.contains('anger'), 3,np.where(df['sentiment'].str.contains('hate'), 4,np.where(df['sentiment'].str.contains('boredom'), 5,np.where(df['sentiment'].str.contains('enthusiasm'), 10,np.where(df['sentiment'].str.contains('neutral'), 6,np.where(df['sentiment'].str.contains('fun'), 8,np.where(df['sentiment'].str.contains('happiness'), 9,np.where(df['sentiment'].str.contains('relief'), 7,np.where(df['sentiment'].str.contains('surprise'), 11,np.where(df['sentiment'].str.contains('love'), 12,0))))))))))))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['content'], \n",
    "                                                    df['Emotion level'],test_size = 0.33,\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36449"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the CountVectorizer to the training data\n",
    "vect = CountVectorizer().fit(X_train)\n",
    "vect.get_feature_names()[::2000]\n",
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " 'alex_navarro',\n",
       " 'beats',\n",
       " 'carleigh',\n",
       " 'cravings',\n",
       " 'dpocza',\n",
       " 'fishhhface',\n",
       " 'hairdressers',\n",
       " 'irishpixie36',\n",
       " 'kirstie',\n",
       " 'madnessofmany',\n",
       " 'mrmiistro',\n",
       " 'overgrown',\n",
       " 'qcy8d4',\n",
       " 'saralds',\n",
       " 'soundtracks',\n",
       " 'theamazon111',\n",
       " 'ustre',\n",
       " 'yellowish']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()[::2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36449"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<26800x36449 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 317981 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectorized = vect.transform(X_train)\n",
    "X_train_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy:  0.342424242424\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "# Predict the transformed test documents\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('Accurarcy: ',accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix\n",
      "[[   0    0    1    0    0    7    1    1   10    1    3    1   12]\n",
      " [   0    0    0    0    2    3    3    1   17    0   11    0   23]\n",
      " [   0    1    1    1    1   14    2    4  145    2   18    3   64]\n",
      " [   0    1    1    1    5   45    3   13   93    1   17    5   54]\n",
      " [   0    0    0    4   29  128    4   36  202    7   41   13  103]\n",
      " [   0    0    2    3   43  586    3  168  520   26   59   44  262]\n",
      " [   0    2    0    0    5   18   66    4   97    0   80    8  125]\n",
      " [   0    0    0    0   17  259    8  497  269   17   63   16  153]\n",
      " [   1    1    6    5   32  227   22  105 1630   20  198   42  582]\n",
      " [   0    0    1    0   12   85    2   29  177   24   27    7  121]\n",
      " [   0    0    4    1   11   79   48   52  433    9  457   15  632]\n",
      " [   0    0    0    1    8  101    9   41  270    6   54   32  198]\n",
      " [   0    0    3    1   19  153   51   71  828   24  413   44 1197]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(cm, names, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(names))\n",
    "    plt.xticks(tick_marks, names, rotation=45)\n",
    "    plt.yticks(tick_marks, names)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix')\n",
    "print(cm)\n",
    "#plt.figure()\n",
    "#plot_confusion_matrix(cm, X_test)\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30363636363636365"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfrNB = naive_bayes.MultinomialNB()\n",
    "clfrNB.fit(X_train_vectorized,y_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "predicted_labels = clfrNB.predict(X_test_vectorized)\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test,predicted_labels,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30363636363636365"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy:  0.303636363636\n"
     ]
    }
   ],
   "source": [
    "print('Accurarcy: ',accuracy_score(y_test, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35325757575757577"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clfrSVM = svm.SVC(kernel='linear', C=0.1)\n",
    "clfrSVM.fit(X_train_vectorized, y_train)\n",
    "predicted_labels = clfrSVM.predict(X_test_vectorized)\n",
    "metrics.f1_score(y_test,predicted_labels,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35325757575757577"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_labels = model_selection.cross_val_predict(clfrSVM,\n",
    "X_train_vectorized, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13200, 26800]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-33f5487660b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Users\\dawit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    690\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    691\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dawit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    804\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    807\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dawit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dawit\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\dawit\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 181\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13200, 26800]"
     ]
    }
   ],
   "source": [
    "metrics.f1_score(y_test,predicted_labels,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest Coefs:\n",
      "['love' 'sick' 'best' 'happy' 'fun' 'actually' 'sad' 'wow' 'amazing' 'wish']\n",
      "\n",
      "Largest Coefs: \n",
      "['donniewahlberg' 'lights' 'weee' 'bored' 'tip' 'exactly' 'stinks' 'sheep'\n",
      " 'complain' 'katy']\n"
     ]
    }
   ],
   "source": [
    "# get the feature names as numpy array\n",
    "feature_names = np.array(vect.get_feature_names())\n",
    "\n",
    "# Sort the coefficients from the model\n",
    "sorted_coef_index = model.coef_[0].argsort()\n",
    "\n",
    "\n",
    "print('Smallest Coefs:\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\n",
    "print('Largest Coefs: \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy:  0.351742424242\n"
     ]
    }
   ],
   "source": [
    "# Fit the TfidfVectorizer to the training data specifiying a minimum document frequency of 5\n",
    "vect = TfidfVectorizer(min_df=10).fit(X_train)\n",
    "len(vect.get_feature_names())\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('Accurarcy: ',accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3175"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfrNB = naive_bayes.MultinomialNB()\n",
    "clfrNB.fit(X_train_vectorized,y_train)\n",
    "X_test_vectorized = vect.transform(X_test)\n",
    "predicted_labels = clfrSVM.predict(X_test_vectorized)\n",
    "metrics.f1_score(y_test,predicted_labels,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3175"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfrSVM = svm.SVC(kernel='linear', C=0.1)\n",
    "clfrSVM.fit(X_train_vectorized, y_train)\n",
    "predicted_labels = clfrSVM.predict(X_test_vectorized)\n",
    "metrics.f1_score(y_test,predicted_labels,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurarcy:  0.325\n"
     ]
    }
   ],
   "source": [
    "# document frequency of 5 and extracting 1-grams and 2-grams\n",
    "vect = CountVectorizer(min_df=5, ngram_range=(1,10)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "predictions = model.predict(vect.transform(X_test))\n",
    "print('Accurarcy: ',accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_vectorized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5e35363dc7a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclfrSVM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclfrSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vectorized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclfrSVM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_vectorized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpredicted_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'micro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_vectorized' is not defined"
     ]
    }
   ],
   "source": [
    "clfrSVM = svm.SVC(kernel='linear', C=0.1)\n",
    "clfrSVM.fit(X_train_vectorized, y_train)\n",
    "predicted_labels = clfrSVM.predict(X_test_vectorized)\n",
    "metrics.f1_score(y_test,predicted_labels,average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(1,2)).fit(X_train)\n",
    "\n",
    "X_train_vectorized = vect.transform(X_train)\n",
    "\n",
    "len(vect.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
